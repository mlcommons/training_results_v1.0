{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype hybrid embedding : data-parallel frequent categories and model- parallel infrequent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "from copy import deepcopy\n",
    "\n",
    "def flatten_data(data):\n",
    "    # concatenate all iterations\n",
    "    samples_data = np.concatenate([deepcopy(data[i][1])\n",
    "                                   for i in range(len(data))], axis=1)\n",
    "\n",
    "    # data dimensions\n",
    "    embedding_sizes = data[0][0]\n",
    "    num_tables = samples_data.shape[0]\n",
    "    num_samples = samples_data.shape[1]\n",
    "\n",
    "    # flatten and make categories unique\n",
    "    samples = np.zeros(num_tables * num_samples, dtype=np.int32)\n",
    "    category_index_offset = 0\n",
    "    for j in range(num_tables):\n",
    "        for i in range(num_samples):\n",
    "            samples[i * num_tables + j] = (category_index_offset\n",
    "                                          + samples_data[j, i])\n",
    "        category_index_offset += embedding_sizes[j]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration - communication measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = [2, 4, 8, 16]\n",
    "\n",
    "# per gpu:\n",
    "D_ar = np.array([0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024] )* 1024*1024\n",
    "T_ar = np.array([\n",
    "    31.940742,\n",
    "    36.368742,\n",
    "    46.126742,\n",
    "    77.696742,\n",
    "    103.154742,\n",
    "    124.293742,\n",
    "    191.450742,\n",
    "    331.715742,\n",
    "    611.883742,\n",
    "    1199.531742,\n",
    "    2225.175742,\n",
    "    4391.540742,\n",
    "    8586.129742])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 nodes\n",
    "# The results of measuring the latencies for varying amount of data\n",
    "# per node?\n",
    "\n",
    "D_a2a_data = []\n",
    "T_a2a_data = []\n",
    "\n",
    "# 2 nodes:\n",
    "T_a2a_data.append(\n",
    "    np.array(\n",
    "        [69.71, 69.98, 67.27, 68.65, 67.98, \n",
    "         68.32, 66.96, 68.03, 67.47, 68.88, \n",
    "         69.02, 69.39, 71.76, 75.59, 84.35, \n",
    "         115.3, 166.4 ,261, 450.7], dtype=np.float64))\n",
    "D_a2a_data.append(\n",
    "    np.array(\n",
    "        [64, 128, 256, 512, 1024, 2048, 4096, \n",
    "         8192, 16384, 32768, 65536, 131072, \n",
    "         262144, 524288, 1048576, 2097152, \n",
    "         4194304, 8388608, 16777216], dtype=np.float64))\n",
    "\n",
    "# 4 nodes:\n",
    "D_a2a_data.append(np.array(\n",
    "    [128, 256, 512, 1024, 2048, 4096, 8192, 16384,\n",
    "     32768, 65536, 131072, 262144, 524288, 1048576, \n",
    "     2097152, 4194304, 8388608, 16777216 \n",
    "                 ], dtype=np.float64))\n",
    "# T_a2a = np.array([116, 101, 101, 112, 101, 99, 103, 100, 102, 101, 270, 109, 107, 117, 159, 230, 369, 690])\n",
    "T_a2a_data.append(np.array(\n",
    "    [116, 101, 101, 112, 101, 99, 103, 100,\n",
    "     102, 101, 105, 109, 107, 117, 159, 230,\n",
    "     369, 690], dtype=np.float64))\n",
    "\n",
    "# 8 nodes:\n",
    "# D_a2a_data.append(np.array(\n",
    "#     [64, 128, 256, 512, 1024, 2048, 4096, 8192, \n",
    "#      16384, 32768, 65536, 131072, 262144, 524288, \n",
    "#      1048576, 2097152, 4194304, 8388608, 16777216], dtype=np.float64))\n",
    "# T_a2a_data.append(np.array(\n",
    "#     [0.14, 0.13, 212.2, 230.9, 201.9, 207.5, 190.4, \n",
    "#      193, 194.8, 187.7, 198.4, 392.8, 190.4, 190, \n",
    "#      212.5, 245.2, 376.4, 487.2, 858.1], dtype=np.float64))\n",
    "D_a2a_data.append(np.array(\n",
    "    [256, 512, 1024, 2048, 4096, 8192, \n",
    "     16384, 32768, 65536, 131072, 262144, 524288, \n",
    "     1048576, 2097152, 4194304, 8388608, 16777216], dtype=np.float64))\n",
    "T_a2a_data.append(np.array(\n",
    "    [212.2, 230.9, 201.9, 207.5, 190.4, \n",
    "     193, 194.8, 187.7, 198.4, 195, 190.4, 190, \n",
    "     212.5, 245.2, 376.4, 487.2, 858.1], dtype=np.float64))\n",
    "\n",
    "# 16 nodes:\n",
    "# D_a2a_data.append(np.array(\n",
    "#     [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, \n",
    "#      32768, 65536, 131072, 262144, 524288, 1048576, \n",
    "#      2097152, 4194304, 8388608, 16777216], dtype=np.float64\n",
    "#      ))\n",
    "# T_a2a_data.append(np.array(\n",
    "#     [0.12, 0.12, 0.13, 445.7, 496.8, 387.9, 397.3, 400.4, \n",
    "#      403.8, 391.3, 408.6, 402.8, 390.1, 406.1, 408.2, 432.2, \n",
    "#      481.4, 1136.2, 1210.3], dtype=np.float64\n",
    "#      ))\n",
    "D_a2a_data.append(np.array(\n",
    "    [512, 1024, 2048, 4096, 8192, 16384, \n",
    "     32768, 65536, 131072, 262144, 524288, 1048576, \n",
    "     2097152, 4194304, 8388608, 16777216], dtype=np.float64\n",
    "     ))\n",
    "T_a2a_data.append(np.array(\n",
    "    [445.7, 496.8, 387.9, 397.3, 400.4, \n",
    "     403.8, 391.3, 408.6, 402.8, 390.1, 406.1, 408.2, 432.2, \n",
    "     481.4, 1136.2, 1210.3], dtype=np.float64\n",
    "     ))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "B_IB = 200e9\n",
    "B_AR = 130e9\n",
    "\n",
    "for i, num_nodes in enumerate(node_list):\n",
    "    D_a2a = D_a2a_data[i]\n",
    "    T_a2a = T_a2a_data[i]\n",
    "\n",
    "    T_sol_a2a = 8 * D_a2a * (num_nodes-1) / num_nodes / B_IB * 1e6\n",
    "    T_sol_ar  = D_ar / B_AR * 1e6\n",
    "\n",
    "    mark_a2a = T_sol_a2a > 10\n",
    "    mask_ar = T_sol_ar > 10\n",
    "\n",
    "    plt.title(f'calibration data {num_nodes} nodes')\n",
    "    plt.plot(D_a2a, T_a2a, 'b.-', label='all-to-all latencies ($\\mu s$)')\n",
    "    plt.plot(D_a2a[mark_a2a], T_sol_a2a[mark_a2a], 'b--', label='all-to-all SOL')\n",
    "    plt.plot(D_ar, T_ar, 'k.-', label='all reduce latencies ($\\mu s$)')\n",
    "    plt.plot(D_ar[mask_ar], T_sol_ar[mask_ar], 'k--', label='all-reduce SOL')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_dim = 1\n",
    "dense_dim = 13\n",
    "slot_num = 26\n",
    "\n",
    "label_dtype = [(f\"label_{i}\", 'f4') for i in range(label_dim)]\n",
    "dense_dtype = [(f\"dense_{i}\", 'f4') for i in range(dense_dim)]\n",
    "sparse_dtype = [(f\"sparse_{i}\", 'i4') for i in range(slot_num)]\n",
    "dataset_dtype = np.dtype(label_dtype + dense_dtype + sparse_dtype)\n",
    "# dataset_fname = \"/raid/datasets/criteo/mlperf/40m.limit_preshuffled/train_data.bin\" # Set dataset path here\n",
    "dataset_fname = \"/datasets/creito/train_data.bin\" # dlcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_dataset(fname):\n",
    "     fp = np.memmap(fname, dtype=dataset_dtype, mode='r')\n",
    "     num_samples = fp.size\n",
    "     print(f\"mmap file done samples: {num_samples}\")\n",
    "     return (fp, num_samples)\n",
    "\n",
    "nmap = memmap_dataset(dataset_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_a_batch(nmap, batch_id, batch_size):\n",
    "    (fp, num_samples) = nmap\n",
    "    max_range = min((batch_id + 1)*batch_size, num_samples)\n",
    "    batch = np.array(fp[batch_id*batch_size : max_range]).tolist()\n",
    "    sparse_batch = [s[label_dim + dense_dim : ] for s in batch]\n",
    "    return np.array([*sparse_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = read_a_batch(nmap, 0, 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 15 batches and format as old reader\n",
    "# Note: temporary to avoid touching too much of the existing code\n",
    "embed_sizes = np.array([39884407, 39043, 17289, 7420, 20263, 3, 7120, 1543,\n",
    "                        63, 38532952, 2953546, 403346, 10, 2208, 11938, 155,\n",
    "                        4, 976, 14, 39979772, 25641295, 39664985, 585935,\n",
    "                        12972, 108, 36])\n",
    "data = []\n",
    "for i in range(15):\n",
    "    data.append((embed_sizes, read_a_batch(nmap, i, 65536).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize frequent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_T(D, T, bytes_in):\n",
    "    # Calibration data of communication times may be noisy data:\n",
    "    #    fit a straight line locally using a Guassion kernel and \n",
    "    #    evaluate the line at D=bytes_in\n",
    "\n",
    "    # if zeros bytes, no communication will be performed and return t = 0\n",
    "    epsilon = 0.5\n",
    "    if bytes_in < epsilon or bytes_in < D[0] / 2:\n",
    "        return 0.\n",
    "\n",
    "    d_max = D[-1]\n",
    "    # width of fit on logarithmic scale\n",
    "    width_log = 2.\n",
    "\n",
    "    num_points = D.size\n",
    "    sigma = (bytes_in * width_log - bytes_in / width_log) / 2.\n",
    "    kernel = np.exp(-(D-bytes_in)**2/(2*sigma*sigma))\n",
    "\n",
    "    # Solve  \n",
    "    #   kernel * ( a D + b ) = kernel * T\n",
    "    # for a and b using linear regression.\n",
    "\n",
    "    M = np.zeros((num_points, 2), dtype=np.float64)\n",
    "    M[:,0] = kernel * D\n",
    "    M[:,1] = kernel\n",
    "\n",
    "    # linear regression using Moore-Penrose inverse \n",
    "    beta = np.dot(np.dot(np.linalg.inv(np.dot(M.T, M)), M.T), kernel * T)\n",
    "\n",
    "    a = beta[0]\n",
    "    b = beta[1]\n",
    "\n",
    "    # evaluate interpolation and return interpolation\n",
    "    t_comm_interpolated = a*bytes_in + b\n",
    "    return t_comm_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, num_nodes in enumerate(node_list):\n",
    "    print(f\"interpolation of calibration data for {num_nodes} nodes\")\n",
    "\n",
    "    D_a2a = D_a2a_data[i]\n",
    "    T_a2a = T_a2a_data[i]\n",
    "    \n",
    "    N = 300\n",
    "    data_a2a = np.linspace(D_a2a[0], D_a2a[-1], N)\n",
    "    t_a2a_int = np.zeros(N)\n",
    "    for i, d in enumerate(data_a2a):\n",
    "        t_a2a_int[i] = interpolate_T(D_a2a, T_a2a, d)\n",
    "    data_ar = np.linspace(D_ar[0], D_ar[-1], N)\n",
    "    t_ar_int = np.zeros(N)\n",
    "    for i, d in enumerate(data_ar):\n",
    "        t_ar_int[i] = interpolate_T(D_ar, T_ar, d)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(D_a2a, T_a2a, 'k.-', label='calibration all-to-all')\n",
    "    plt.plot(data_a2a, t_a2a_int, label='interpolation all-to-all')\n",
    "\n",
    "    plt.plot(D_ar, T_ar, 'b.-', label='calibration all-reduce')\n",
    "    plt.plot(data_ar, t_ar_int, 'r', label='interpolation all reduce')\n",
    "\n",
    "    plt.title('calibration data')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('number of bytes per gpu')\n",
    "    plt.ylabel('communication time ($\\mu s$)')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    plt.subplot(122)\n",
    "\n",
    "    # plt.plot(D_a2a, T_a2a, 'k.-', label='calibration all-to-all')\n",
    "    plt.plot(data_a2a, t_a2a_int, label='interpolation all-to-all')\n",
    "\n",
    "    # plt.plot(D_ar, T_ar, 'b.-', label='calibration all-reduce')\n",
    "    plt.plot(data_ar, t_ar_int, 'r', label='interpolation all reduce')\n",
    "\n",
    "    plt.title('interpolation of data')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('number of bytes per gpu')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_frequent_categories(data, num_nodes, embedding_parameters, calibration_data):\n",
    "    num_batches = len(data)\n",
    "    batch_size = data[0][1].shape[1]\n",
    "    embedding_sizes = data[0][0]\n",
    "    num_tables = embedding_sizes.size\n",
    "    \n",
    "    #print('initializing frequent categories..')\n",
    "    \n",
    "    # get samples and category counts\n",
    "    #print('flattening data..')\n",
    "    samples = flatten_data(data)\n",
    "    categories, counts = np.unique(samples, return_counts=True)\n",
    "    \n",
    "    #print('performing stats..')\n",
    "    # sort counts and categories from most frequent to least frequent\n",
    "    index_count_sort = np.argsort(counts)\n",
    "    categories_sort = deepcopy(categories[index_count_sort])[::-1]\n",
    "    counts_sort = deepcopy(counts[index_count_sort])[::-1]\n",
    "\n",
    "    num_samples = num_tables * num_batches * batch_size\n",
    "    ## plot stats:\n",
    "    # plt.plot(np.cumsum(counts_sort) / num_samples*100, label='frequent categories')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # embedding parameters\n",
    "    embedding_vec_size = embedding_parameters.embedding_vec_size\n",
    "    data_element_size = embedding_parameters.data_element_size\n",
    "\n",
    "    # calibration data\n",
    "    D_a2a = calibration_data.D_a2a # unit : data in bytes per gpu total the all-to-all message size (# ranks x size)\n",
    "    T_a2a = calibration_data.T_a2a # unit : time in microseconds\n",
    "    D_ar = calibration_data.D_ar   # unit : data in bytes per gpu\n",
    "    T_ar = calibration_data.T_ar   # unit : time in microseconds\n",
    "\n",
    "    # some theoretical maxima\n",
    "    B_a2a_max = 190e9\n",
    "    B_ar_max = 230e9\n",
    "\n",
    "    n_max = 0.1 * B_a2a_max / B_ar_max * num_nodes / (num_nodes-1)\n",
    "    # node occupancy of the categories\n",
    "    n_c = counts_sort / (num_batches * num_nodes)\n",
    "    num_frequent_max = (np.argmax(n_c < n_max) + 1)\n",
    "\n",
    "    #print('calculating communication times..')\n",
    "    # calculate the communication times for all possible number of \n",
    "    # frequent categories up to num_frequent_max\n",
    "    communication_time = np.zeros(num_frequent_max)\n",
    "    comm_time_ar = np.zeros(num_frequent_max)\n",
    "    comm_time_a2a = np.zeros(num_frequent_max)\n",
    "    for num_frequent_categories in range(num_frequent_max):\n",
    "\n",
    "        # calculate all-to-all bytes\n",
    "        bytes_a2a = (num_samples - np.sum(counts_sort[:num_frequent_categories]) ) * embedding_vec_size * data_element_size\n",
    "        bytes_a2a_gpu = bytes_a2a / (num_batches * num_nodes) / 8\n",
    "\n",
    "        # calculate all-reduce bytes\n",
    "        bytes_ar = num_frequent_categories * embedding_vec_size * data_element_size\n",
    "\n",
    "        t_ar = interpolate_T(D_ar, T_ar, bytes_ar)\n",
    "        t_a2a = 2*interpolate_T(D_a2a, T_a2a, bytes_a2a_gpu)\n",
    "\n",
    "        # record data\n",
    "        comm_time_ar[num_frequent_categories] = t_ar\n",
    "        comm_time_a2a[num_frequent_categories] = t_a2a\n",
    "        communication_time[num_frequent_categories] = t_ar + t_a2a\n",
    "\n",
    "    num_frequent = int(np.argmin(communication_time) + 1)\n",
    "    return num_frequent, categories_sort, counts_sort, communication_time, comm_time_ar, comm_time_a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "class EmbeddingParameters:\n",
    "    def __init__(self, embedding_vec_size=128, data_element_size=2):\n",
    "        self.embedding_vec_size = embedding_vec_size\n",
    "        self.data_element_size = data_element_size\n",
    "\n",
    "class CalibrationData:\n",
    "    def __init__(self, D_ar, T_ar, D_a2a, T_a2a):\n",
    "        self.D_ar = D_ar\n",
    "        self.T_ar = T_ar\n",
    "        self.D_a2a = D_a2a\n",
    "        self.T_a2a = T_a2a\n",
    "\n",
    "import matplotlib\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "font = {'family' : 'normal',\n",
    "    'weight' : 'normal',\n",
    "    'size'   : 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, num_nodes in enumerate(node_list):\n",
    "\n",
    "    D_a2a = D_a2a_data[i]\n",
    "    T_a2a = T_a2a_data[i]\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(f'{color.BOLD}{color.GREEN}Hybrid embedding communication optimization on {num_nodes} nodes{color.END}')\n",
    "\n",
    "    N = 300\n",
    "    data_a2a = np.linspace(D_a2a[0], D_a2a[-1], N)\n",
    "    t_a2a_int = np.zeros(N)\n",
    "    for i, d in enumerate(data_a2a):\n",
    "        t_a2a_int[i] = interpolate_T(D_a2a, T_a2a, d)\n",
    "    data_ar = np.linspace(D_ar[0], D_ar[-1], N)\n",
    "    t_ar_int = np.zeros(N)\n",
    "    for i, d in enumerate(data_ar):\n",
    "        t_ar_int[i] = interpolate_T(D_ar, T_ar, d)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(D_a2a, T_a2a, 'k.-', label='calibration all-to-all')\n",
    "    plt.plot(data_a2a, t_a2a_int, label='interpolation all-to-all')\n",
    "\n",
    "    plt.plot(D_ar, T_ar, 'b.-', label='calibration all-reduce')\n",
    "    plt.plot(data_ar, t_ar_int, 'r', label='interpolation all reduce')\n",
    "\n",
    "    plt.title('calibration data')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('number of bytes per gpu')\n",
    "    plt.ylabel('communication time ($\\mu s$)')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    embedding_vec_size = 128\n",
    "    data_element_size = 2\n",
    "    num_frequent_categories, frequent_categories, counts_frequent_categories, communication_time, comm_time_ar, comm_time_a2a \\\n",
    "        = initialize_frequent_categories(\n",
    "        data, num_nodes,\n",
    "        EmbeddingParameters(embedding_vec_size=embedding_vec_size, data_element_size=data_element_size),\n",
    "        CalibrationData(D_ar, T_ar, D_a2a, T_a2a))\n",
    "    num_frequent = num_frequent_categories\n",
    "    num_frequent_max = communication_time.size\n",
    "    t_min = communication_time[num_frequent-1]\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title('intitialization frequent categories - communication time')\n",
    "    plt.text(num_frequent-1, t_min+100, f'comm time = {t_min:4.0f} microseconds')\n",
    "    plt.plot(range(num_frequent_max), communication_time, 'k-', label=f'communication time, num_frequent = {num_frequent:3,d}')\n",
    "    plt.plot(range(num_frequent_max), comm_time_ar, 'r--', label=f'communication time all-reduce')\n",
    "    plt.plot(range(num_frequent_max), comm_time_a2a, 'b--', label=f'communication time all-to-all')\n",
    "    plt.plot(range(num_frequent_max), t_min*np.ones(num_frequent_max), 'k--')\n",
    "    plt.xlabel('number of frequent categories')\n",
    "    plt.ylabel('communication time ($\\mu s$)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    counts = counts_frequent_categories\n",
    "    num_batches = len(data)\n",
    "    batch_size = data[0][1].shape[1]\n",
    "    num_tables = data[0][1].shape[0]\n",
    "    num_samples = batch_size * num_batches * num_tables\n",
    "    percentage_samples_ar = np.sum(counts[:num_frequent_categories]) / num_samples * 100\n",
    "    print(f'speedup hybrid model vs model-parallel : {color.BOLD}{communication_time[0] / communication_time[num_frequent_categories-1]:4.2f} X{color.END}')\n",
    "    print()\n",
    "    print(f'number of frequent categories = {color.BOLD}{color.BLUE}{num_frequent_categories:3,d}{color.END}')\n",
    "    print(f'total communication time      = {color.BOLD}{color.BLUE}{communication_time[num_frequent_categories-1]:4.0f} microseconds {color.END}(vs {communication_time[0]:4.0f} microseconds)')\n",
    "    print()\n",
    "    print(f'samples covered by all-reduce{color.BOLD} (data-parallel){color.END}  = {percentage_samples_ar:2.1f} %')\n",
    "    print(f'samples covered by all-to-all{color.BOLD} (model-parallel){color.END} = {100-percentage_samples_ar:2.1f} %')\n",
    "    print()\n",
    "    bytes_ar = num_frequent_categories * embedding_vec_size * data_element_size\n",
    "    print(f'bytes per gpu into all-reduce : {int(bytes_ar):8,d} bytes per gpu')\n",
    "    bytes_a2a = (num_samples - np.sum(counts[:num_frequent_categories]) ) * embedding_vec_size * data_element_size\n",
    "    bytes_a2a_gpu = bytes_a2a / (num_batches * num_nodes) / 8\n",
    "    print(f'bytes per gpu into all-to-all : {int(bytes_a2a):8,d} bytes per gpu, equivalent rank size = {int(bytes_ar/(num_nodes*8)):6,d} bytes')\n",
    "    print(f'')\n",
    "    print()\n",
    "    t_ar = interpolate_T(D_ar, T_ar, bytes_ar)\n",
    "    latency_ar = (t_ar*1e-6 - bytes_ar / 130e9)*1e6\n",
    "    print(f'all-reduce communication time : {t_ar:3.1f} microseconds,\\x1b[31m latency = {latency_ar:3.1f} microseconds\\x1b[0m (assuming 130 GB/s algo bandwidth)')\n",
    "    t_a2a = interpolate_T(D_a2a, T_a2a, bytes_a2a_gpu)\n",
    "    latency_a2a = (t_a2a*1e-6 - bytes_a2a_gpu*(num_nodes-1)/num_nodes / 24e9)*1e6\n",
    "    print(f'all-to-all communication time : {t_a2a:3.1f} microseconds,\\x1b[31m latency = {latency_a2a:3.1f} microseconds\\x1b[0m (assuming 24 GB/s NIC-IB bandwidth)')\n",
    "    print()\n",
    "    print(f'latency all-reduce + 2 x latency all-to-all = {color.BOLD}\\x1b[31m {latency_ar + 2*latency_a2a:3.1f} microseconds latency total\\x1b[0m on {num_nodes}{color.END} nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize data-structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure nodes and gpus\n",
    "\n",
    "class Gpu:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.frequent_categories = None\n",
    "        self.category_frequent_index = None\n",
    "        self.frequent_embedding_vectors = None\n",
    "        self.frequent_partial_gradients = None\n",
    "        self.category_location = None\n",
    "        self.node = None\n",
    "\n",
    "    def init_embedding_cache(self, num_frequent, embedding_vec_size):\n",
    "        self.num_frequent = num_frequent\n",
    "        self.frequent_embedding_vectors = np.zeros(num_frequent*embedding_vec_size, dtype=np.float32)\n",
    "        self.frequent_partial_gradients = np.zeros(num_frequent*embedding_vec_size, dtype=np.float32)\n",
    "        \n",
    "class Node:\n",
    "\n",
    "    def __init__(self, num_gpus):\n",
    "        self.gpus = [Gpu() for i in range(num_gpus)]\n",
    "        for i in range(num_gpus):\n",
    "            self.gpus[i].gpu_id = i\n",
    "            self.gpus[i].node = self # reference to this node\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, nodes):\n",
    "        self.nodes = nodes\n",
    "\n",
    "    def all_reduce(self):\n",
    "        pass\n",
    "\n",
    "    def all_to_all(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup nodes, gpus and network:\n",
    "i_node = 1 # 4 nodes\n",
    "num_nodes = node_list[i_node]\n",
    "\n",
    "nodes = [Node(8) for i in range(num_nodes)]\n",
    "gpus = [gpu for node in nodes for gpu in node.gpus]\n",
    "num_gpus = len(gpus)\n",
    "for i in range(num_nodes):\n",
    "    nodes[i].node_id = i\n",
    "network = Network(nodes)\n",
    "\n",
    "for node in nodes:\n",
    "    print(f\"Node {node.node_id} with gpu's {[gpu.gpu_id for gpu in node.gpus]}, reporting for duty!\")\n",
    "for gpu in gpus:\n",
    "    print(f\"Gpu {gpu.gpu_id} on node {gpu.node.node_id}, reporting for duty!\")\n",
    "\n",
    "print(f\"network with nodes {[node.node_id for node in network.nodes]} reporting for duty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{color.BOLD}{color.GREEN}Hybrid embedding communication optimization on {num_nodes} nodes{color.END}')\n",
    "print()\n",
    "print(f'Setting up data structures for run on {num_nodes} nodes')\n",
    "\n",
    "D_a2a = D_a2a_data[i_node]\n",
    "T_a2a = T_a2a_data[i_node]\n",
    "\n",
    "print(f'Initializing frequent categories..')\n",
    "embedding_vec_size = 128\n",
    "data_element_size = 2\n",
    "num_frequent_categories, frequent_categories, counts_frequent_categories, communication_time, comm_time_ar, comm_time_a2a \\\n",
    "    = initialize_frequent_categories(\n",
    "    data, num_nodes,\n",
    "    EmbeddingParameters(embedding_vec_size=embedding_vec_size, data_element_size=data_element_size),\n",
    "    CalibrationData(D_ar, T_ar, D_a2a, T_a2a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frequent = num_frequent_categories\n",
    "t_min = communication_time[num_frequent-1]\n",
    "print(f\"Number of frequent categories = {color.BOLD}{num_frequent:5,d}{color.END}, embedding communication time {color.BOLD}{t_min:4.0f}{color.END} microseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category_frequent_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_sizes = data[0][0]\n",
    "num_tables = embedding_sizes.size\n",
    "num_categories = np.sum(embedding_sizes)\n",
    "print(f'Total number of categories : {num_categories:8,d}, category_frequent_index array size : {num_categories*4/(1024*1024):4.2f} MB')\n",
    "category_frequent_index = num_categories * np.ones(num_categories, dtype=np.int32)\n",
    "frequent_categories = frequent_categories[:num_frequent]\n",
    "\n",
    "# initializing category_frequent_index :\n",
    "category_frequent_index[frequent_categories] = np.array(range(num_frequent), dtype=np.int32)\n",
    "\n",
    "# this array is identical on all gpu's :\n",
    "for gpu in gpus:\n",
    "    gpu.category_frequent_index = category_frequent_index\n",
    "\n",
    "n_display = 20\n",
    "print(f'{color.BOLD}{color.RED}category          |-> frequent category cache index{color.END}')\n",
    "for category in range(n_display):\n",
    "    frequent_category_cache_index = category_frequent_index[category]\n",
    "    if frequent_category_cache_index < num_categories:\n",
    "        print(f'category {color.BOLD}{category:3d}{color.END}      |->  cache index {color.BOLD}{color.BLUE}{frequent_category_cache_index:6,d}{color.END}')\n",
    "    else:\n",
    "        print(f'category {color.BOLD}{category:3d}{color.END}      |->  cache index    {color.BOLD}{color.RED}END{color.END}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_frequent_index.shape\n",
    "np.sum(category_frequent_index < num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize frequent_embedding_vectors\n",
    "# initialize frequent_partial_gradients\n",
    "for gpu in gpus:\n",
    "    gpu.init_embedding_cache(num_frequent, embedding_vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes a lot of time, there are many infrequent categories! (15 minutes)\n",
    "\n",
    "# category_location\n",
    "num_infrequent = num_categories - num_frequent\n",
    "category_location = num_categories * np.ones((num_categories,2), dtype=np.int32)\n",
    "#locations_infrequent = [ [int(np.floor(i/8)),i%8] for i in range(num_infrequent) ]\n",
    "infrequent_index = np.zeros(num_categories)\n",
    "infrequent_index[category_frequent_index == num_categories] = range(num_infrequent)\n",
    "for c in range(num_categories):\n",
    "    if category_frequent_index[c] == num_categories:\n",
    "        index = infrequent_index[c]\n",
    "        category_location[c,:] = [index % num_gpus, index // num_gpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in gpus:\n",
    "    gpu.category_location = category_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_display = 20\n",
    "print(f'{color.BOLD}{color.RED}category          |->  category location {color.END}')\n",
    "for category in range(n_display):\n",
    "    location = category_location[category,:]\n",
    "    if location[0] < num_categories:\n",
    "        print(f'category {color.BOLD}{category:3d}{color.END}      |->  category location {color.BOLD}{color.GREEN}{location}{color.END}')\n",
    "    else:\n",
    "        print(f'category {color.BOLD}{category:3d}{color.END}      |->  category location   {color.BOLD}{color.RED}END{color.END}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-node: node_id, gpu_id\n",
    "# single-node: gpu_id, category_model_index\n",
    "\n",
    "# linear location index:\n",
    "#\n",
    "# multi-node: node_id * 8 + gpu_id\n",
    "# single-node: gpu_id * max_model_size + category_model_index\n",
    "\n",
    "# sample 3 in network 5: category 7 stored in model (gpu) 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HybridEmbedding: \n",
    "#\n",
    "#   frequent_categories_\n",
    "#   category_frequent_index_\n",
    "#   category_location_\n",
    "#\n",
    "#   frequent_embedding_;\n",
    "#   infrequent_embeddin_;\n",
    "\n",
    "# Forward: sample |-> batch embedding vectors in mlp (concatenation)\n",
    "#\n",
    "#    data-parallel (all-reduce), model-parallel (all-to-all)\n",
    "#\n",
    "\n",
    "#\n",
    "# sample : 26 fields categorical feature => category\n",
    "#          embedding vector category 0, embedding vector category 1, embedding vector category 2, ...\n",
    "\n",
    "# EmbeddingFrequent ( data-parallel, all-reduce )\n",
    "#   \n",
    "#   frequent_embedding_vectors\n",
    "#   frequent_partial_gradients\n",
    "#\n",
    "#   stores the frequent embedding \n",
    "#   update: reduces locally the frequent gradients into the frequent_partial_gradients array\n",
    "#      frequent sample indices: update_sgd(sample_indices_frequent, samples_frequent_index, gradients_samples)\n",
    "#\n",
    "#   all-reduce : frequent_partial_gradients\n",
    "#   update the frequent_embedding_vectors\n",
    "#\n",
    "\n",
    "# EmbeddingInfrequent => \n",
    "#\n",
    "#   store the infrequent categories\n",
    "#\n",
    "#   infrequent_embedding_vectors\n",
    "#\n",
    "#   \n",
    "#   # local batch, global batch\n",
    "#   \n",
    "#   all-to-all forward\n",
    "#      send buffer: \n",
    "#         (I) entire batch |-> mark samples' categories that is placed here\n",
    "#         create list of indices for entire batch, of samples' embedding vector to send\n",
    "#         create offset per destination\n",
    "#      receive buffer: \n",
    "#         (II) local mlp - batch |-> sort by source ( doesn't need to be sort )\n",
    "#\n",
    "#   all-to-all backward\n",
    "#      send buffer: (II)\n",
    "#      receive buffer: (I)\n",
    "#\n",
    "#   update infrequent embedding vectors\n",
    "#      (I) |-> categories |-> where stored? samples_infrequent_index\n",
    "#      update_sgd(sample_indices_infrequent, samples_infrequent_index, infrequent_embedding_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def get_node_gpu(node_id, gpu_id):\n",
    "    # not efficient, but that's not the point here! :P\n",
    "    node = None\n",
    "    gpu = None\n",
    "    for node_ in nodes:\n",
    "        if node_.node_id == node_id:\n",
    "            node = node_\n",
    "            break\n",
    "    for gpu_ in node.gpus:\n",
    "        if gpu_.gpu_id == gpu_id:\n",
    "            gpu = gpu_\n",
    "            break\n",
    "    return node, gpu\n",
    "\n",
    "def get_network_id(node_id, gpu_id):\n",
    "    for i in range(len(gpus)):\n",
    "        if gpus[i].node.node_id == node_id and gpus[i].gpu_id == gpu_id:\n",
    "            return i\n",
    "    raise KeyError(f\"Not found: node {node_id}, GPU {gpu_id}\")\n",
    "\n",
    "def cub_DeviceSelect(gpu, samples, network_id):\n",
    "    location = gpu.category_location[samples,:]\n",
    "    samples_mask = (location[:,0] == network_id)\n",
    "    samples_filter = np.r_[:samples.size][samples_mask]\n",
    "    return samples_filter\n",
    "\n",
    "# model indices: forward-send, backward-receive\n",
    "def calculate_model_indices(samples, node_id, gpu_id):\n",
    "    _, gpu = get_node_gpu(node_id, gpu_id)\n",
    "    network_id = get_network_id(node_id, gpu_id)\n",
    "    section_size = samples.size // num_gpus\n",
    "\n",
    "    sample_model_indices = cub_DeviceSelect(gpu, samples, network_id)\n",
    "    network_offset_model_indices = np.zeros(num_gpus, dtype=np.int32)\n",
    "    for i in range(num_gpus):\n",
    "        network_offset_model_indices[i] = bisect_left(sample_model_indices, i * section_size)\n",
    "\n",
    "    return sample_model_indices, network_offset_model_indices\n",
    "\n",
    "# network indices: forward-receive, backward-send\n",
    "def calculate_network_indices(samples, node_id, gpu_id):\n",
    "    _, gpu = get_node_gpu(node_id, gpu_id)\n",
    "\n",
    "    section_size = samples.size // num_gpus\n",
    "    network_id = get_network_id(node_id, gpu_id)\n",
    "    start_idx = network_id * section_size\n",
    "    end_idx = min((network_id + 1) * section_size, samples.size)\n",
    "    sub_batch = samples[start_idx:end_idx]\n",
    "\n",
    "    location = gpu.category_location[sub_batch,:]\n",
    "    samples_mask = location[:,0] < num_categories\n",
    "    infrequent_indices = deepcopy(np.r_[:sub_batch.size][samples_mask])\n",
    "    network_indices = deepcopy(location[:, 0][samples_mask])\n",
    "    sorted_indices = np.array(sorted(zip(network_indices, infrequent_indices),\n",
    "                                     key=lambda x: x[0]))\n",
    "\n",
    "    sample_network_offsets = np.zeros(num_gpus, dtype=np.int32)\n",
    "    if len(network_indices):\n",
    "        sample_network_indices = sorted_indices[:,1]\n",
    "        for i in range(num_gpus):\n",
    "            sample_network_offsets[i] = bisect_left(sorted_indices[:,0], i)\n",
    "    else:\n",
    "        sample_network_indices = np.zeros(0)\n",
    "    \n",
    "    return sample_network_indices, sample_network_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "batch = flatten_data([data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indices = {}\n",
    "model_indices_offsets = {}\n",
    "for node_ in nodes:\n",
    "    for gpu_ in node_.gpus:\n",
    "        node_id = node_.node_id\n",
    "        gpu_id = gpu_.gpu_id\n",
    "        idx, off = calculate_model_indices(batch, node_id, gpu_id)\n",
    "        model_indices[(node_id, gpu_id)] = idx\n",
    "        model_indices_offsets[(node_id, gpu_id)] = off\n",
    "\n",
    "# print(model_indices)\n",
    "# print(model_indices_offsets)\n",
    "\n",
    "network_indices = {}\n",
    "network_indices_offsets = {}\n",
    "for node_ in nodes:\n",
    "    for gpu_ in node_.gpus:\n",
    "        node_id = node_.node_id\n",
    "        gpu_id = gpu_.gpu_id\n",
    "        idx, off = calculate_network_indices(batch, node_id, gpu_id)\n",
    "        network_indices[(node_id, gpu_id)] = idx\n",
    "        network_indices_offsets[(node_id, gpu_id)] = off\n",
    "\n",
    "# print(network_indices)\n",
    "# print(network_indices_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent sample indices\n",
    "def calculate_frequent_sample_indices(samples, node_id, gpu_id):\n",
    "    _, gpu = get_node_gpu(node_id, gpu_id)\n",
    "    \n",
    "    section_size = samples.size // num_gpus\n",
    "    network_id = get_network_id(node_id, gpu_id)\n",
    "    start_idx = network_id * section_size\n",
    "    end_idx = min((network_id + 1) * section_size, samples.size)\n",
    "    sub_batch = samples[start_idx:end_idx]\n",
    "\n",
    "    freq_indices = gpu.category_frequent_index[sub_batch]\n",
    "    samples_mask = freq_indices < num_categories\n",
    "    frequent_sample_indices = deepcopy(np.r_[:sub_batch.size][samples_mask])\n",
    "    \n",
    "    return frequent_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_sample_indices = {}\n",
    "for node_ in nodes:\n",
    "    for gpu_ in node_.gpus:\n",
    "        node_id = node_.node_id\n",
    "        gpu_id = gpu_.gpu_id\n",
    "        frequent_sample_indices[(node_id, gpu_id)] = \\\n",
    "            calculate_frequent_sample_indices(batch, node_id, gpu_id)\n",
    "\n",
    "# print(frequent_sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computation takes quite a long time\n",
    "# TODO: numpify more for better performance\n",
    "def calculate_model_cache_indices(samples, node_id, gpu_id):\n",
    "    _, gpu = get_node_gpu(node_id, gpu_id)\n",
    "    \n",
    "    section_size = samples.size // num_gpus\n",
    "    model_id = get_network_id(node_id, gpu_id)\n",
    "    num_frequent_per_model = num_frequent // num_gpus\n",
    "    \n",
    "    # Compute the mask\n",
    "    network_frequent_mask = np.zeros(num_gpus * num_frequent_per_model, dtype=bool)\n",
    "    for i in range(num_gpus):\n",
    "        freq_index = gpu.category_frequent_index[\n",
    "            samples[i * section_size:(i+1)*section_size]]\n",
    "        for idx in freq_index:\n",
    "            if idx < num_frequent and idx // num_frequent_per_model == model_id:\n",
    "                network_frequent_mask[i * num_frequent_per_model\n",
    "                                      + idx % num_frequent_per_model] = True\n",
    "  \n",
    "    # Select categories according to the mask\n",
    "    model_cache_indices = np.r_[:num_gpus * num_frequent_per_model][network_frequent_mask]\n",
    "    \n",
    "    # Compute offsets\n",
    "    model_cache_indices_offsets = np.zeros(num_gpus + 1, dtype=np.int32)\n",
    "    for i in range(num_gpus):\n",
    "        model_cache_indices_offsets[i] = bisect_left(model_cache_indices, i * num_frequent_per_model)\n",
    "    model_cache_indices_offsets[num_gpus] = model_cache_indices.size\n",
    "    \n",
    "    # Convert to buffer indices\n",
    "    model_cache_indices = (model_cache_indices % num_frequent_per_model\n",
    "                           + model_id * num_frequent_per_model)\n",
    "    \n",
    "    return model_cache_indices, model_cache_indices_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_indices = {}\n",
    "model_cache_indices_offsets = {}\n",
    "for node_ in nodes:\n",
    "    for gpu_ in node_.gpus:\n",
    "        node_id = node_.node_id\n",
    "        gpu_id = gpu_.gpu_id\n",
    "        idx, off = calculate_model_cache_indices(batch, node_id, gpu_id)\n",
    "        model_cache_indices[(node_id, gpu_id)] = idx\n",
    "        model_cache_indices_offsets[(node_id, gpu_id)] = off\n",
    "\n",
    "# print(model_cache_indices)\n",
    "# print(model_cache_indices_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_network_cache_indices(samples, node_id, gpu_id):\n",
    "    _, gpu = get_node_gpu(node_id, gpu_id)\n",
    "    model_id = get_network_id(node_id, gpu_id)\n",
    "    network_mask = np.zeros(num_frequent, dtype=bool)\n",
    "    section_size = samples.size // num_gpus\n",
    "    sample_mlp_batch = samples[model_id * section_size:min(samples.size,(model_id + 1)*section_size)]\n",
    "    freq_index = gpu.category_frequent_index[sample_mlp_batch]\n",
    "    for index in freq_index:\n",
    "        if index < num_frequent:\n",
    "            network_mask[index] = True\n",
    "    network_cache_indices = np.r_[:num_frequent][network_mask]\n",
    "    \n",
    "    # Compute offsets\n",
    "    num_frequent_per_model = num_frequent // num_gpus\n",
    "    network_cache_indices_offsets = np.zeros(num_gpus + 1, dtype=np.int32)\n",
    "    for i in range(num_gpus):\n",
    "        network_cache_indices_offsets[i] = bisect_left(network_cache_indices, i * num_frequent_per_model)\n",
    "    network_cache_indices_offsets[num_gpus] = network_cache_indices.size\n",
    "\n",
    "    return network_cache_indices, network_cache_indices_offsets, network_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_cache_indices = {}\n",
    "network_cache_indices_offsets = {}\n",
    "network_cache_masks = {}\n",
    "for node_ in nodes:\n",
    "    for gpu_ in node_.gpus:\n",
    "        node_id = node_.node_id\n",
    "        gpu_id = gpu_.gpu_id\n",
    "        idx, off, mask = calculate_network_cache_indices(batch, node_id, gpu_id)\n",
    "        network_cache_indices[(node_id, gpu_id)] = idx\n",
    "        network_cache_indices_offsets[(node_id, gpu_id)] = off\n",
    "        network_cache_masks[(node_id, gpu_id)] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model forward:\n",
    "\n",
    "node_id = 0\n",
    "gpu_id = 1\n",
    "network_id = get_network_id(node_id, gpu_id)\n",
    "\n",
    "samples = flatten_data([data[0]])\n",
    "# location_samples = category_location[samples,:]\n",
    "# sample_lin_location_index = np.zeros(samples.size, dtype=np.int32)\n",
    "# for i, category in enumerate(samples):\n",
    "#     if category_location[category,0] < num_categories:\n",
    "#         lin_location_index = category_location[category,0]*8 + category_location[category,1]\n",
    "#         sample_lin_location_index[i] = lin_location_index\n",
    "#     else:\n",
    "#         sample_lin_location_index[i] = num_categories\n",
    "# model_lin_index = node_id * 8 + gpu_id\n",
    "# samples_mask = (sample_lin_location_index == model_lin_index)\n",
    "samples_mask = category_location[samples,0] == network_id\n",
    "samples_mask\n",
    "np.sum(samples_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Backward receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 16\n",
    "55*1024*26*128*2/(nn*nn*64)/4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read 15 batches of 64k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_variable(lines, indx):\n",
    "    line_i = lines[indx]\n",
    "    line_split = line_i.split()\n",
    "    variable_name = line_split[0]\n",
    "    num_data = int(line_split[1])\n",
    "    if num_data == 1:\n",
    "        offset = 0\n",
    "        if len(line_split) == 3:\n",
    "            data = np.int64(line_split[2])\n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = 1\n",
    "            data = np.int64(lines[indx+1])\n",
    "        return variable_name, data, indx + 1 + offset\n",
    "    else:\n",
    "        values = np.zeros(num_data, dtype=np.int64)\n",
    "        for i in range(num_data):\n",
    "            values[i] = np.int64(lines[indx+1+i])\n",
    "        return variable_name, values, indx + 1 + num_data\n",
    "\n",
    "def read_dlrm_data(folder_name):\n",
    "    data = {}\n",
    "\n",
    "    file_names = os.listdir(folder_name)\n",
    "    for file_name in file_names:\n",
    "        print(file_name)\n",
    "        split_list = file_name.split(\"_\")\n",
    "        iteration = int(split_list[2])\n",
    "        gpu_id = int(split_list[4].split(\".\")[0])\n",
    "\n",
    "        # parse file\n",
    "        fobj = open(os.path.join(folder_name, file_name), \"r\")\n",
    "        lines = fobj.readlines()\n",
    "        indx = 0\n",
    "        _, num_samples, indx = read_variable(lines, indx)\n",
    "\n",
    "        _, slot_num, indx = read_variable(lines, indx)\n",
    "        _, size_embedding, indx = read_variable(lines, indx)\n",
    "        size_embeddings = size_embedding.astype(np.int64)\n",
    "        _, categories_raw, indx = read_variable(lines, indx)\n",
    "\n",
    "        categories = np.zeros( (slot_num, num_samples), dtype=np.int64 )\n",
    "        if slot_num > 1:\n",
    "            for i in range(categories_raw.size):\n",
    "                offset = 0\n",
    "                if slot_num > 1:\n",
    "                    offset = np.sum(size_embeddings[:i%slot_num])\n",
    "                #print(f\"i mod 2 : {i%2}, i/2 : {np.int(np.floor(i / 2))}\")\n",
    "                categories[(i%slot_num), int(np.floor(i / 2))] = categories_raw[i] - offset\n",
    "        else:\n",
    "            categories = categories_raw.reshape((1, categories_raw.size))\n",
    "\n",
    "        data[(iteration, gpu_id)] = (num_samples, size_embeddings, categories)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"/mnt/c/Users/dabel/Documents/mlperf/data/\")\n",
    "data = read_dlrm_data(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = {}\n",
    "# concatenate embeddings per iteration\n",
    "for iteration, gpu in data:\n",
    "    if not iteration in iterations:\n",
    "        iterations[iteration] = [gpu]\n",
    "    else:\n",
    "        iterations[iteration].append(gpu)\n",
    "\n",
    "iteration_numbers = [num for num in iterations]\n",
    "iteration_numbers = np.sort(iteration_numbers)\n",
    "#print(len(iteration_numbers), iteration_numbers)\n",
    "\n",
    "samples_iteration = []\n",
    "for iteration in iteration_numbers:\n",
    "    i_table = 0\n",
    "    embedding_sizes = []\n",
    "    data_iteration = np.zeros( (26, 65536), dtype=np.int64 )\n",
    "    for gpu in range(16):\n",
    "        size_embeddings_gpu = data[(iteration, gpu)][1]\n",
    "        num_tables_gpu = size_embeddings_gpu.size\n",
    "        for i_table_gpu in range(num_tables_gpu):\n",
    "            data_iteration[i_table,:] = data[(iteration, gpu)][2][i_table_gpu,:]\n",
    "            if num_tables_gpu > 1:\n",
    "                embedding_sizes.append(size_embeddings_gpu[i_table_gpu])\n",
    "            else:\n",
    "                embedding_sizes.append(size_embeddings_gpu)\n",
    "            i_table += 1\n",
    "    samples_iteration.append( (np.array(embedding_sizes), data_iteration) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_iteration[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "samples = np.concatenate([deepcopy(samples_iteration[i][1]) for i in range(len(samples_iteration))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = samples_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
